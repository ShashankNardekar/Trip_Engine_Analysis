{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dd8029-4eb6-4322-8bb8-6804814ebf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kruskal_h_test(df,target_col,col):\n",
    "    \n",
    "    h_statistic, p_value = stats.kruskal(*[group[target_col] for name, group in df.groupby(col)])\n",
    "    \n",
    "    results_df = pd.DataFrame({\n",
    "        'categorical_column': [col],\n",
    "        'p_value': [p_value]\n",
    "    })\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "\n",
    "def error_rate_table(df,target_bucket,col,col_name,x,y,order_flag,ord_list,search_filter):\n",
    "    \n",
    "    \n",
    "    total_searches = df.groupby(col)['createtripid_region'].count().rename('total_searches')\n",
    "    \n",
    "    \n",
    "    total_errors = df.groupby(col)[target_bucket].sum().rename('target_errors')\n",
    "    \n",
    "    \n",
    "    searches_errors_df = pd.concat([total_searches, total_errors], axis=1)\n",
    "\n",
    "    searches_errors_df['normalized_error_rate'] = searches_errors_df['target_errors'] / searches_errors_df['total_searches']\n",
    "    searches_errors_df = searches_errors_df.sort_values('normalized_error_rate', ascending=False)\n",
    "\n",
    "    searches_mean = searches_errors_df['total_searches'].mean()\n",
    "\n",
    "    if search_filter == True: # filter by search mean for true error rate values\n",
    "        searches_errors_df = searches_errors_df[(searches_errors_df['normalized_error_rate'] > 0) & (searches_errors_df['total_searches'] >= searches_mean)].sort_values('normalized_error_rate', ascending=False)\n",
    "    else:\n",
    "        searches_errors_df = searches_errors_df[(searches_errors_df['normalized_error_rate'] > 0)].sort_values('normalized_error_rate', ascending=False)\n",
    "\n",
    "    # only show top 25 rows by error rate\n",
    "    searches_errors_df = searches_errors_df[:25]\n",
    "\n",
    "    searches_errors_df['searches_mean'] = searches_mean\n",
    "\n",
    "    groups = searches_errors_df.reset_index()[:5][col]\n",
    "\n",
    "    return groups\n",
    "\n",
    "def error_rate_table_all(df,target_bucket,col,col_name,x,y,order_flag,ord_list,search_filter):\n",
    "    \n",
    "    \n",
    "    total_searches = df.groupby(col)['createtripid_region'].count().rename('total_searches')\n",
    "    \n",
    "    total_errors = df.groupby(col)[target_bucket].sum().rename('target_errors')\n",
    "    \n",
    "    searches_errors_df = pd.concat([total_searches, total_errors], axis=1)\n",
    "\n",
    "    \n",
    "    searches_errors_df['normalized_error_rate'] = searches_errors_df['target_errors'] / searches_errors_df['total_searches']\n",
    "    searches_errors_df = searches_errors_df.sort_values('normalized_error_rate', ascending=False)\n",
    "\n",
    "    searches_mean = searches_errors_df['total_searches'].mean()\n",
    "\n",
    "    if search_filter == True: # filter by search mean for true error rate values\n",
    "        searches_errors_df = searches_errors_df[(searches_errors_df['normalized_error_rate'] > 0) & (searches_errors_df['total_searches'] >= searches_mean)].sort_values('normalized_error_rate', ascending=False)\n",
    "    else:\n",
    "        searches_errors_df = searches_errors_df[(searches_errors_df['normalized_error_rate'] > 0)].sort_values('normalized_error_rate', ascending=False)\n",
    "\n",
    "    # only show top 25 rows by error rate\n",
    "    searches_errors_df = searches_errors_df[:25]\n",
    "\n",
    "    searches_errors_df['searches_mean'] = searches_mean\n",
    "\n",
    "\n",
    "    return searches_errors_df\n",
    "\n",
    "\n",
    "def error_rate(df,target_bucket,col,col_name,x,y,order_flag,ord_list,search_filter):\n",
    "    \n",
    "    total_searches = df.groupby(col)['createtripid_region'].count().rename('total_searches')\n",
    "    \n",
    "    total_errors = df.groupby(col)[target_bucket].sum().rename('target_errors')\n",
    "    \n",
    "    searches_errors_df = pd.concat([total_searches, total_errors], axis=1)\n",
    "\n",
    "    \n",
    "    searches_errors_df['normalized_error_rate'] = searches_errors_df['target_errors'] / searches_errors_df['total_searches']\n",
    "    searches_errors_df = searches_errors_df.sort_values('normalized_error_rate', ascending=False)\n",
    "\n",
    "    searches_mean = searches_errors_df['total_searches'].mean()\n",
    "\n",
    "    if search_filter == True: # filter by search mean for true error rate values\n",
    "        searches_errors_df = searches_errors_df[(searches_errors_df['normalized_error_rate'] > 0) & (searches_errors_df['total_searches'] >= searches_mean)].sort_values('normalized_error_rate', ascending=False)\n",
    "    else:\n",
    "        searches_errors_df = searches_errors_df[(searches_errors_df['normalized_error_rate'] > 0)].sort_values('normalized_error_rate', ascending=False)\n",
    "\n",
    "    # only show top 25 rows by error rate\n",
    "    searches_errors_df = searches_errors_df[:25]\n",
    "\n",
    "    error_data = searches_errors_df['normalized_error_rate'].to_dict()\n",
    "    \n",
    "    def custom_sort_key(item):\n",
    "        key, value = item\n",
    "        return ord_list.index(key)\n",
    "        \n",
    "    if order_flag == True:\n",
    "        error_data = dict(sorted(error_data.items(), key=custom_sort_key, reverse=False))\n",
    "\n",
    "    # Creating a horizontal bar chart\n",
    "    plt.figure(figsize=(x,y))\n",
    "    plt.barh(list(error_data.keys()), list(error_data.values()), color=plt.cm.viridis(np.linspace(0, 1, len(df))))\n",
    "    plt.xlabel(target_bucket + ' rate')\n",
    "    plt.ylabel(col_name)\n",
    "    plt.title(target_bucket+' rate by '+col_name)\n",
    "    plt.gca().invert_yaxis()  \n",
    "    plt.show()\n",
    "\n",
    "    searches_errors_df['searches_mean'] = searches_mean\n",
    "\n",
    "    return searches_errors_df\n",
    "\n",
    "def stacked_heatmap(df,target_buckets,col,col_name,x,y,flag,order_flag,ord_list):\n",
    "\n",
    "    all_cols = [col]+target_buckets\n",
    "    \n",
    "    new_df = df[[col]].drop_duplicates()\n",
    "    \n",
    "    for target_bucket in target_buckets:\n",
    "        total_searches = df.groupby(col, observed=False)['createtripid_region'].count().rename('total_searches')\n",
    "        \n",
    "        total_errors = df.groupby(col, observed=False)[target_bucket].sum().rename('target_errors')\n",
    "        \n",
    "        grouped_data = pd.concat([total_searches, total_errors], axis=1)\n",
    "        \n",
    "        grouped_data[target_bucket + '_rate'] = grouped_data['target_errors'] / grouped_data['total_searches']\n",
    "        \n",
    "        new_df = pd.merge(new_df, grouped_data[[target_bucket + '_rate']],\n",
    "                          left_on=col, right_index=True, suffixes=('', '_'+target_bucket))\n",
    "        new_df = new_df.reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    new_df[col] = pd.Categorical(new_df[col], categories=ord_list, ordered=True)\n",
    "    \n",
    "    new_df = new_df.sort_values(by=col)\n",
    "    \n",
    "    new_df.set_index(col, inplace=True)\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(new_df, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Error Types')\n",
    "    plt.title('Error Rate Heatmap')\n",
    "    plt.show()\n",
    "\n",
    "    return new_df\n",
    "\n",
    "def grouped_error_rate_3(target_bucket,cols,col_names,x,y):\n",
    "    total_searches = df.groupby(cols)['createtripid_region'].count().rename('total_searches')\n",
    "    \n",
    "    total_errors = df.groupby(cols)[target_bucket].sum().rename('target_errors')\n",
    "    \n",
    "    searches_errors_df = pd.concat([total_searches, total_errors], axis=1)\n",
    "    \n",
    "    searches_errors_df['normalized_error_rate'] = searches_errors_df['target_errors'] / searches_errors_df['total_searches']\n",
    "  \n",
    "    searches_mean = searches_errors_df['total_searches'].mean()\n",
    "    \n",
    "    filtered_df = searches_errors_df[(searches_errors_df['normalized_error_rate'] > 0) & (searches_errors_df['total_searches'] >= searches_mean)].sort_values('normalized_error_rate', ascending=False)\n",
    "\n",
    "    filtered_df['searches_mean'] = searches_mean\n",
    "    \n",
    "    filtered_df_reset = filtered_df.reset_index()\n",
    "    \n",
    "    filtered_df_reset['group_label'] = filtered_df_reset.apply(lambda row: f\"{row[cols[0]]}, {row[cols[1]]}, {row[cols[2]]}\", axis=1)\n",
    "\n",
    "    filtered_df_reset = filtered_df_reset[:25]\n",
    "    \n",
    "    plt.figure(figsize=(x,y))\n",
    "    sns.barplot(x='normalized_error_rate', y='group_label', data=filtered_df_reset, palette=\"viridis\")\n",
    "    \n",
    "    plt.xlabel(target_bucket)\n",
    "    plt.ylabel(col_names)\n",
    "    plt.title(target_bucket+' Error Rate by '+col_names)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    return filtered_df_reset\n",
    "\n",
    "def grouped_error_rate_2(df,target_bucket,cols,col_names,x,y):\n",
    "    total_searches = df.groupby(cols)['createtripid_region'].count().rename('total_searches')\n",
    "    \n",
    "    total_errors = df.groupby(cols)[target_bucket].sum().rename('target_errors')\n",
    "    \n",
    "    searches_errors_df = pd.concat([total_searches, total_errors], axis=1)\n",
    "    \n",
    "    searches_errors_df['normalized_error_rate'] = searches_errors_df['target_errors'] / searches_errors_df['total_searches']\n",
    "    \n",
    "    searches_mean = searches_errors_df['total_searches'].mean()\n",
    "    \n",
    "    filtered_df = searches_errors_df[(searches_errors_df['normalized_error_rate'] > 0) & (searches_errors_df['total_searches'] >= searches_mean)].sort_values('normalized_error_rate', ascending=False)\n",
    "\n",
    "    filtered_df['searches_mean'] = searches_mean\n",
    "    \n",
    "    filtered_df_reset = filtered_df.reset_index()\n",
    "    \n",
    "    filtered_df_reset['group_label'] = filtered_df_reset.apply(lambda row: f\"{row[cols[0]]}, {row[cols[1]]}\", axis=1)\n",
    "\n",
    "    filtered_df_reset = filtered_df_reset[:25]\n",
    "    \n",
    "    plt.figure(figsize=(x,y))\n",
    "    sns.barplot(x='normalized_error_rate', y='group_label', data=filtered_df_reset, palette=\"viridis\")\n",
    "    \n",
    "    plt.xlabel(target_bucket)\n",
    "    plt.ylabel(col_names)\n",
    "    plt.title(target_bucket+' Error Rate by '+col_names)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    return filtered_df_reset\n",
    "\n",
    "\n",
    "def grouped_error_rate_2_table(df,target_bucket,cols,col_names,x,y):\n",
    "    total_searches = df.groupby(cols)['createtripid_region'].count().rename('total_searches')\n",
    "    \n",
    "    total_errors = df.groupby(cols)[target_bucket].sum().rename('target_errors')\n",
    "    \n",
    "    searches_errors_df = pd.concat([total_searches, total_errors], axis=1)\n",
    "    \n",
    "    searches_errors_df['normalized_error_rate'] = searches_errors_df['target_errors'] / searches_errors_df['total_searches']\n",
    "    \n",
    "    searches_mean = searches_errors_df['total_searches'].mean()\n",
    "    \n",
    "    filtered_df = searches_errors_df[(searches_errors_df['normalized_error_rate'] > 0) & (searches_errors_df['total_searches'] >= searches_mean)].sort_values('normalized_error_rate', ascending=False)\n",
    "\n",
    "    filtered_df['searches_mean'] = searches_mean\n",
    "    \n",
    "    filtered_df_reset = filtered_df.reset_index()\n",
    "    \n",
    "    filtered_df_reset['group_label'] = filtered_df_reset.apply(lambda row: f\"{row[cols[0]]}, {row[cols[1]]}\", axis=1)\n",
    "\n",
    "    filtered_df_reset = filtered_df_reset[:25]\n",
    "\n",
    "    return filtered_df_reset\n",
    "\n",
    "\n",
    "def heatmap2(df_tmp,filtered_df_reset,col1,col2,target_bucket,order):\n",
    "    df_tmp = df_tmp[['createtripid_region',col1,col2,target_bucket]]\n",
    "    filtered_df_reset = filtered_df_reset.rename(columns={'group_label':col1})\n",
    "    \n",
    "    filtered_df_reset = filtered_df_reset[:5][[col1]]\n",
    "    considered_group = pd.merge(filtered_df_reset,df_tmp,on=col1)\n",
    "    \n",
    "    total_searches = considered_group.groupby([col1,col2],observed=True)['createtripid_region'].count().rename('total_searches')\n",
    "    \n",
    "    total_errors = considered_group.groupby([col1,col2])[target_bucket].sum().rename('target_errors')\n",
    "    \n",
    "    grouped_data = pd.concat([total_searches, total_errors], axis=1)\n",
    "    \n",
    "    grouped_data[target_bucket + '_rate'] = grouped_data['target_errors'] / grouped_data['total_searches']\n",
    "    \n",
    "    grouped_data = grouped_data.reset_index()\n",
    "    \n",
    "    grouped_data[col2] = pd.Categorical(grouped_data[col2], categories=order, ordered=True)\n",
    "\n",
    "    pivot_df = grouped_data.pivot_table(values=target_bucket + '_rate',\n",
    "                              index=col2,\n",
    "                              columns=col1,\n",
    "                              aggfunc='sum', fill_value=0)\n",
    "    pivot_df = pivot_df.T\n",
    "    # Create the heatmap\n",
    "    plt.figure(figsize=(10, 4))  # Adjust the figure size as needed\n",
    "    sns.heatmap(pivot_df, cmap='viridis', annot=True, fmt='.2f', linewidths=0.5)\n",
    "    \n",
    "    plt.xlabel(col2)\n",
    "    plt.ylabel(col1)\n",
    "    plt.title(target_bucket+' rate Heatmap')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    return pivot_df\n",
    "\n",
    "\n",
    "# converting children ages column: -1 for no child and mean of 1 or more children ages \n",
    "def convert_empty_brackets_and_mean(value):\n",
    "    if value == '{}':\n",
    "        return 'no child'#-1\n",
    "    else:\n",
    "        set_value = eval(value)  # Convert the string to a set\n",
    "        if len(set_value) > 0:\n",
    "            return sum(set_value) / len(set_value)\n",
    "        else:\n",
    "            return 'no child'#-1\n",
    "\n",
    "\n",
    "def column_conversions(df):\n",
    "    \n",
    "    df['success'] = df['success'].replace({True: 1, False: 0})\n",
    "    \n",
    "    # converting array_to_string column to theme preference columns (one hot encoded)\n",
    "    array_to_string_df = (df['array_to_string'].str.split(r'\\s*,\\s*', expand=True)\n",
    "       .apply(pd.Series.value_counts, 1)\n",
    "       .iloc[:, 1:]\n",
    "       .fillna(0, downcast='infer'))\n",
    "    \n",
    "    df = pd.concat([df, array_to_string_df], axis=1)\n",
    "    \n",
    "    df.drop(columns=['array_to_string'], inplace=True)\n",
    "    \n",
    "    \n",
    "    df['childages'] = df['childages'].apply(convert_empty_brackets_and_mean)\n",
    "    df['childages'] = df['childages'].astype(str)\n",
    "    \n",
    "    # # temporal columns\n",
    "    \n",
    "    df['request_weekday'] = df['requestdate'].dt.weekday\n",
    "    df['request_hour'] = df['requestdate'].dt.hour.astype(str)\n",
    "    \n",
    "    df['creation_weekday'] = df['creationdate'].dt.weekday\n",
    "    df['creation_hour'] = df['creationdate'].dt.hour.astype(str)\n",
    "    \n",
    "    # Extract year, quarter, month, and weekday from 'preferreddate'\n",
    "    df['preferred_year'] = df['preferreddate'].dt.year.astype(str)\n",
    "    df['preferred_quarter'] = df['preferreddate'].dt.quarter.astype(str)\n",
    "    df['preferred_month'] = df['preferreddate'].dt.month\n",
    "    df['preferred_weekday'] = df['preferreddate'].dt.weekday\n",
    "    \n",
    "    # converting date columns to day names\n",
    "    df['request_weekday'] = df['request_weekday'].apply(lambda x: calendar.day_name[x])\n",
    "    df['creation_weekday'] = df['creation_weekday'].apply(lambda x: calendar.day_name[x])\n",
    "    df['preferred_weekday'] = df['preferred_weekday'].apply(lambda x: calendar.day_name[x])\n",
    "    \n",
    "    # converting preferreddate column to month name \n",
    "    df['preferred_month'] = df['preferred_month'].astype(int).apply(lambda x: calendar.month_name[x])\n",
    "    \n",
    "    df['days_between'] = (df['preferreddate'] - df['requestdate']).dt.days\n",
    "\n",
    "    return df\n",
    "\n",
    "def bucketizing_columns(df):\n",
    "    df['request_hour'] = df['request_hour'].astype(int)\n",
    "    df['creation_hour'] = df['creation_hour'].astype(int)\n",
    "    df['childages'] = df['childages'].replace('no child',-1)\n",
    "    df['childages'] = df['childages'].astype(float)\n",
    "    \n",
    "    # # creation_hour\n",
    "    df.loc[df['childages'] ==-1, 'child_ages_bucket'] = 'no child'\n",
    "    df.loc[(df['childages'] > 0)&(df['childages'] <= 6), 'child_ages_bucket'] = 'child age 0 to 6'\n",
    "    df.loc[(df['childages'] > 6)&(df['childages'] <= 10), 'child_ages_bucket'] = 'child age 7 to 10'\n",
    "    df.loc[(df['childages'] > 10), 'child_ages_bucket'] = 'child age 10+'\n",
    "    \n",
    "    bins = [2, 6, 7, 11, 18, 25]  # Defining the bin edges based on the logical bins \n",
    "    labels = ['Short Trips', 'Medium Short Trips', 'Medium Trips', 'Medium Long Trips', 'Long Trips']  # Bin labels\n",
    "    \n",
    "    df['trip_duration_bucket'] = pd.cut(df['durationdays'], bins=bins, labels=labels, right=True)\n",
    "    \n",
    "    \n",
    "    df['failtime_bucket'] = pd.cut(df['failtime'],\n",
    "                                        bins=[-float('inf'), 0, 5, 10, float('inf')],\n",
    "                                        labels=['NaN', '1 to 5 seconds', '6 to 10 seconds', '10+ seconds'])\n",
    "    \n",
    "    df['tripbuildtime_bucket'] = pd.cut(df['tripbuildtime'],\n",
    "                                        bins=[-float('inf'),0, 5, 15, 30,60,float('inf')],\n",
    "                                        labels=['NaN','buildtime 0 to 5', 'buildtime 6 to 15', 'buildtime 16 to 30','buildtime 31 to 60', 'buildtime >60'])\n",
    "    \n",
    "    \n",
    "    df['request_hour_bucket'] = pd.cut(df['request_hour'],\n",
    "                                        bins=[0,5, 10, 15, 24],\n",
    "                                        labels=['12am to 5am', '5am to 10am', '10am to 3pm','3pm to 12am'],right=False, include_lowest=True)\n",
    "    \n",
    "    df['creation_hour_bucket'] = pd.cut(df['creation_hour'],\n",
    "                                        bins=[0,5, 10, 15, 24],\n",
    "                                        labels=['12am to 5am', '5am to 10am', '10am to 3pm','3pm to 12am'],right=False, include_lowest=True)\n",
    "    \n",
    "    \n",
    "    bins = [7, 37, 58, 132, 193, 350, 2979] \n",
    "    labels = ['Very Short', 'Short to Medium', 'Medium',\n",
    "              'Medium to Long', 'Long', 'Very Long']\n",
    "    \n",
    "    df['lead_time_category'] = pd.cut(df['days_between'], bins=bins, labels=labels, right=False)\n",
    "    \n",
    "    # converting below columns to string columns (for analysis against errors)\n",
    "    \n",
    "    to_str_cols = ['adultcount', 'childcount', 'roomcount',\n",
    "     'Arts and Culture', 'Food and Drink', 'Good for kids', 'History', 'NA', 'Nature', 'Nightlife', 'Relaxation', 'preferred_year', 'preferred_quarter',\n",
    "     'preferred_month']\n",
    "    \n",
    "    for col in to_str_cols:\n",
    "        df[col] = df[col].astype(str)\n",
    "    \n",
    "    categ_cols = ['region','closestcity','twinroompreferred',\n",
    "                 'Arts and Culture', 'Food and Drink', 'Good for kids', 'History', 'NA',\n",
    "           'Nature', 'Nightlife', 'Relaxation','adultcount','childcount','roomcount',\n",
    "    'request_weekday','request_hour','creation_weekday','creation_hour','preferred_year','preferred_quarter','preferred_month',\n",
    "    'preferred_weekday','child_ages_bucket','trip_duration_bucket','tripbuildtime_bucket','lead_time_category']\n",
    "    \n",
    "    # 'not considered : failtime_bucket'\n",
    "    \n",
    "    categorical_cols = to_str_cols + categ_cols\n",
    "    categorical_cols = list(set(categorical_cols))\n",
    "\n",
    "    return df,categorical_cols\n",
    "\n",
    "def combined_kruskal_function(df,columns_to_analyze,errors_rearranged):\n",
    "    \n",
    "    combined_kruskal_test = pd.DataFrame(columns=['categorical_column'])\n",
    "    \n",
    "    for target_col in errors_rearranged:\n",
    "        all_results_df = pd.DataFrame(columns=['categorical_column', 'p_value'])\n",
    "        \n",
    "        for col in columns_to_analyze:\n",
    "            results_df = kruskal_h_test(df, target_col, col)\n",
    "            all_results_df = pd.concat([all_results_df, results_df], ignore_index=True)\n",
    "        \n",
    "        kruskal_df = all_results_df.set_index('categorical_column')\n",
    "        kruskal_df = kruskal_df.rename(columns={'p_value':target_col})\n",
    "        combined_kruskal_test = pd.concat([combined_kruskal_test,kruskal_df],axis=1)\n",
    "    \n",
    "    combined_kruskal_test = combined_kruskal_test.drop(columns=['categorical_column'])\n",
    "\n",
    "    return combined_kruskal_test\n",
    "\n",
    "def plot_combined_kruskal_plot(combined_kruskal_test):\n",
    "    threshold = 0.05\n",
    "    \n",
    "    # Function to calculate color intensity based on value\n",
    "    def calculate_color_intensity(value, threshold):\n",
    "        if value >= threshold:\n",
    "            return 1  # Green\n",
    "        else:\n",
    "            # Calculate a scaled value between 0 (just below threshold) and -1 (for minimum value, assuming 0 as min for simplicity)\n",
    "            return (value / threshold) - 1\n",
    "    \n",
    "    # Apply the function to normalize data\n",
    "    norm_data_adjusted = combined_kruskal_test.applymap(lambda x: calculate_color_intensity(x, threshold))\n",
    "    \n",
    "    # Creating a new heatmap with adjusted normalization\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(norm_data_adjusted, cmap=sns.diverging_palette(10, 133, sep=80, n=50), cbar=True,annot=True)\n",
    "    \n",
    "    # Customizing the plot\n",
    "    plt.title(\"Adjusted Custom Heatmap\")\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    \n",
    "    # Show the adjusted plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return combined_kruskal_test\n",
    "\n",
    "def top_5_category_errors(combined_kruskal_test_t,category):\n",
    "    \n",
    "    error_top_5_categs = pd.DataFrame()\n",
    "    high_error_cols = combined_kruskal_test_t[combined_kruskal_test_t[category]<=0.05].index.tolist()\n",
    "    \n",
    "    for error in high_error_cols:\n",
    "        top_5_vals = error_rate_table(df,error,category,category,4,3,False,order,False)\n",
    "        top_5_vals = top_5_vals.reset_index(drop=False).rename(columns={category:error})\n",
    "        del top_5_vals['index']\n",
    "        error_top_5_categs = pd.concat([error_top_5_categs,top_5_vals],axis=1)\n",
    "    return error_top_5_categs\n",
    "\n",
    "def aggregated_months(df1,col,error_types):\n",
    "    df1 = df1[error_types+[col]]\n",
    "    \n",
    "    aggregated_data = df1.groupby(col).sum()\n",
    "    \n",
    "    aggregated_data.reset_index(inplace=True)\n",
    "    \n",
    "    colors = [\"#00798c\", \"#d1495b\", '#edae49', '#66a182']\n",
    "    \n",
    "    fig, axs = plt.subplots(len(error_types), 1, figsize=(12, 8), sharex=True)\n",
    "    \n",
    "    \n",
    "    for i, error_type in enumerate(error_types):\n",
    "        axs[i].bar(aggregated_data[col], aggregated_data[error_type], color=colors[i], label=error_type)\n",
    "        axs[i].set_title(error_type)\n",
    "        axs[i].tick_params(labelrotation=45)\n",
    "        axs[i].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def outlier_counts(df,error_columns):\n",
    "    \n",
    "    outlier_info = {}\n",
    "    \n",
    "    for column in error_columns:\n",
    "        Q1 = df[column].quantile(0.25)\n",
    "        Q3 = df[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "        outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "        outlier_info[column] = {\n",
    "            \"lower_bound\": lower_bound,\n",
    "            \"upper_bound\": upper_bound,\n",
    "            \"outliers_count\": outliers.shape[0]\n",
    "        }\n",
    "    \n",
    "    outlier_info_df = pd.DataFrame(outlier_info).transpose()\n",
    "    \n",
    "    return outlier_info_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
